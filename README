--撒花）欢迎90src的领导前来视察工作！(撒花--
-署名-方大核桃
-邮箱-fiht@vip.qq.com
备注：此爬虫基于python3,外部模块需要安装requests库 pip3 install request <linux>
大致思路：
1.爬取网页上所有链接：按规则分成友情链接，可能存在sql注入的链接，无用的链接（如 /hello/css.css 即为无用链接）
2.顺着友情链接继续，检测每一个友情链接页面上的友链，可能存在sql注入的链接，无用的链接
3.循环往复。将可能存在payload的链接交给sqlmap检测（sqlmapapi）
技术实现难点：
1.url去重规则
2.友链爬取的深度
3.sqlmap的扫描速度跟不上

使用方法：
为了开发方便，检测目标网页需要在源代码中更改参数，具体位置已在源代码中标明

此爬虫现在已经实现的功能：
给定网站，检测参数，递归查找给定网站的可能存在sql注入的地方（可能存在get型注入的地方）--是全站链接url中存在参数的网页，并非脚本经过检查发现sql注入的网页，检测sql注入会在近期推出

下一步要实现的（大概）：
结合sqlmap实际检测，因为输出的payload的网页不重复，故优化逻辑可暂时放下

2016-03-27 11:01:25 提交了一次：
1.添加了友情链接的去重模块，把之前的self.friend_url 变成了一个全局dic
2.顺着链接下去找的功能 需要爬虫记录已经检测过的url(避免死循环)，由于使用单一数据结构太麻烦，可能会使用数据库（mysql来完成）
3.上次考虑的类的定义问题将变成数据库的设计问题
4.怎样控制爬虫的深度和广度依旧是一个难以解决的问题，暂时的想法是当等待sqlmap确认的url到了一定数目线程就睡眠，但这个显然不科学
5.下一步要做的是设计数据库和重新定义这个脚本的类以及方法

2016-03-26 21:27:25 提交了一次：
1.改善了url的去重算法
2.友链定义存在问题（怎样算友链？同路径带http路径的链接须去除重复值）
3.类的定义问题（上次提交的时候发现对整个扫描体系的把握有点不够，代码十分之凌乱）
4.怎样控制友链的深度和广度

2016-03-25 18:21:15 提交了一次：
1.修复了一些逻辑问题
2.url去重算法有待完善
3.友链需要加入更多的模块（已经扫描？只留下主机名字？）
4.控制友链的深度以及广度

2016-03-25 13:06:20 提交了一次：
目前实现的功能十分简陋
对面向对象的认知不够强大导致代码有点乱
此次提交实现了url参数中payload的简单去重
下一步要做的是使用sqlmapapi实现对目标url的检测

2016-03-24 23：21：20提交了一次：
目前实现的功能还十分简陋
在下一次push前应该完成
1.对友链的规范处理，由于有些友链中依旧存在类似id=XXX此类的payload,所以应该规范，友链应该只保留网站名+路径。
2.去除重复url和重复友情链接
