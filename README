github 用得不是很熟悉
加个README文件记住现在的开发进度
--欢迎90src的领导前来视察工作！--
2016-03-27 11:01:25 提交了一次：
1.添加了友情链接的去重模块，把之前的self.friend_url 变成了一个全局dic
2.顺着链接下去找的功能 需要爬虫记录已经检测过的url(避免死循环)，由于使用单一数据结构太麻烦，可能会使用数据库（mysql来完成）
3.上次考虑的类的定义问题将变成数据库的设计问题
4.怎样控制爬虫的深度和广度依旧是一个难以解决的问题，暂时的想法是当等待sqlmap确认的url到了一定数目线程就睡眠，但这个显然不科学
5.下一步要做的是设计数据库和重新定义这个脚本的类以及方法

2016-03-26 21:27:25 提交了一次：
1.改善了url的去重算法
2.友链定义存在问题（怎样算友链？同路径带http路径的链接须去除重复值）
3.类的定义问题（上次提交的时候发现对整个扫描体系的把握有点不够，代码十分之凌乱）
4.怎样控制友链的深度和广度

2016-03-25 18:21:15 提交了一次：
1.修复了一些逻辑问题
2.url去重算法有待完善
3.友链需要加入更多的模块（已经扫描？只留下主机名字？）
4.控制友链的深度以及广度

2016-03-25 13:06:20 提交了一次：
目前实现的功能十分简陋
对面向对象的认知不够强大导致代码有点乱
此次提交实现了url参数中payload的简单去重
下一步要做的是使用sqlmapapi实现对目标url的检测

2016-03-24 23：21：20提交了一次：
目前实现的功能还十分简陋
在下一次push前应该完成
1.对友链的规范处理，由于有些友链中依旧存在类似id=XXX此类的payload,所以应该规范，友链应该只保留网站名+路径。
2.去除重复url和重复友情链接
